.. _design_uc02 :

Conduct On-demand Processing
============================

This use case aims at using TEP Urban for submitting processing on demand driven by the Area of interest search or the data selection


On demand data processing from portal
-------------------------------------

Some dataset can be asked on demand by the user to the production center.

Next figure depicts the sequence that consists of requesting a processing on demand based on a previous dataset selection as described in :ref:`design_uc01_dds`.


.. uml::
  :caption: On demand data processing submission
  :align: center


  actor "User" as U
  participant "Portal" as P
  database "Portal database" as PDB
  participant "Web Processing Service" as WPS

  U -> P : Select processing service
  P <-> PDB : Load service endpoint
  ... see WPS Service Analysis Sequence Diagram - Describe Process ...
  P -> U : Processing service description (input, output...)

  alt direct remote WPS

  U -> WPS : submit WPS execute request
  activate WPS
  WPS -> U : WPS process id
  deactivate WPS
  U -> P : save processing job information
  activate P
  P -> PDB : save job
  P -> U : return job id
  deactivate P

  else proxied remote WPS

  U -> P : submit WPS execute request
  activate P
  P -> P : proxy WPS request
  activate P #DarkSalmon
  P -> WPS : submit WPS execute request
  activate WPS
  WPS -> P : WPS process id
  deactivate WPS
  deactivate P
  P -> U : WPS process id
  U -> P : save processing job information
  P -> PDB : save job
  P -> U : return job id
  deactivate P

  end


.. warning:: 
  
  This scenario illustration does not take into account the quota and credit management on purpose for readability reason. All details about accounting operations that may apply to this scenario are described in the :ref:`dynamic_accounting` section.



The previous sequence does not list all interactions with the Web Processing Service. Its intent is to show how the portal handles a proxied services or not and how it manages the job history in the database. Next section describes exhaustively all the other intertactions to support, job monitoring and job completion.



Processing execution in the processing centers
----------------------------------------------


.. uml::
  :caption: Processing execution in the processing centers - part 1
  :align: center


  participant "Portal"
  participant "WPS"
  participant "Production Control" as PC
  participant "Scheduler"
  participant "Processor"
  participant "Staging"

  alt synchronous

	  activate Scheduler
	  Portal -> WPS : Execute request
	  
	  activate WPS
	  WPS -> PC : send job request
	  
	  activate PC
	  PC -> Scheduler : send processing request
	  
	  Scheduler -> Processor : perform job
	  
	  activate Processor
	  PC -> PC : start monitoring thread
	  activate PC #Green
	  PC -> Processor : check if the job is finished
	  Processor -> PC : job status
	  note right : job in progress
	  
	  Processor -> Staging : send the products
	  deactivate Processor
	  activate Staging
	  
	  PC -> Processor : check if the job is finished
	  Processor -> PC : job status
	  note right #00B200 : <color:white>job finished</color>
	  
	  PC -> Staging : check if the staging is finished
	  Staging -> PC : staging status
	  note right : staging in progress
	  deactivate Staging
	  
	  PC -> Staging : check if the staging is finished
	  Staging -> PC : product URL(s) and metadata
	  note right #00B200 : <color:white>staging finished</color>
	  
	  PC -> WPS : product URL(s)
	  deactivate PC
	  deactivate PC
	  
	  WPS -> Portal : product URL(s)
	  deactivate WPS

  end

  

.. uml::
  :caption: Processing execution in the processing centers - part 2
  :align: center

  alt asynchronous

	  Portal -> WPS : Execute request
	  activate WPS
	  WPS -> PC : send job request
	  
	  activate PC
	  PC -> Scheduler : send processing request
	  Scheduler -> PC : job ID
	  
	  PC -> WPS : job ID
	  WPS -> Portal : job ID
	  deactivate WPS
	  
	  Scheduler -> Processor : perform job
	  
	  activate Processor
	  PC -> PC : start monitoring thread
	  activate PC #Green
	  
	  Portal -> WPS : GetStatus request
	  activate WPS
	  WPS -> PC : check if the job is finished
	  
	  PC -> Processor : check if the job is finished
	  Processor -> PC : job progress
	  note right : job in progress
	  PC -> WPS : job progress
	  WPS -> Portal : job progress
	  deactivate WPS
	  
	  Processor -> Staging : send the products
	  deactivate Processor
	  activate Staging
	  
	  Portal -> WPS : GetStatus request
	  activate WPS
	  WPS -> PC : check if the job is finished
	  
	  PC -> Processor : check if the job is finished
	  Processor -> PC : job progress
	  note right #00B200 : <color:white>job finished</color>
	  PC -> Staging : check if the staging is finished
	  Staging -> PC : staging status
	  note right : staging in progress
	  PC -> WPS : job progress
	  WPS -> Portal : job progress
	  deactivate WPS
	  
	  
	  deactivate Staging
	  
	  Portal -> WPS : GetStatus request
	  activate WPS
	  WPS -> PC : check if the job is finished
	  
	  PC -> Processor : check if the job is finished
	  Processor -> PC : job progress
	  note right #00B200 : <color:white>job finished</color>
	  PC -> Staging : check if the staging is finished
	  Staging -> PC : product URL(s) and metadata
	  note right #00B200 : <color:white>staging finished</color>
	  PC -> WPS : product URL(s)
	  deactivate PC
	  deactivate PC
	  WPS -> Portal : product URL(s)
	  deactivate WPS
	  

  end

There are two types of processing mode that are supported by the processing centers that are configurable by an attribute field ``status`` in WPS execute requests. 
These two modes are **synchronous** and **asynchronous**, as displayed in the sequence diagram above. 

In **synchronous** mode, when an execute request is received from the Portal, WPS creates a job request based on the information provided in the execute request. 
The job request is then sent to Production Control. Subsequently, the job request is propagated to Scheduler while at the same time a monitoring thread is created at Production Control to monitor the job progress.
The job request is now in the Scheduler queue. The scheduler handles all the incoming processing requests with different job complexity and required processors. Depending on the configuration of each processor, it is possible to perform multiple concurrrent processing in every processor. 
When a resource has become available in the target processor, the request is then sent to the specific processor, as indicated in the execute request. 
When the processing job is finished, the staging job is started. During the processing and staging, the monitoring thread of Production Control actively inquires the status of the job in a pre-configured time interval.
When the staging job has been completed, product URL(s) and metadata information are returned to the Production Control. The product URL(s) is sent to WPS, and eventually to the Portal (wrapped in a WPS response XML). 
The metadata is processed and sent to Catalogue. More details on the product registration to catalogue can be found here :ref:`product_to_catalogue`. It is worth mentioning that the WPS session is alive as long as the
job is still ongoing. The session is killed after an indication from the Processor or Staging that the job has been completed (successfully or unsuccessfully) has been received.

In **asynchronous** mode, the initial procedure is the same as the synchronous up to the job submission to the Scheduler. While nothing is returned back to the WPS in synchronous mode, a job ID is returned back to WPS and eventually to the Portal in asynchronous mode. The WPS session terminates right after this. What happens between Production Control, Scheduler, Processor, and Staging are identical to the one in synchronous mode, but the main difference is that Portal can now inquire the status of a job at anytime, given the job ID. Each call (GetStatus) is represented by a short session with an almost immediate response back to the Portal. As it is in the synchronous mode, product URL(s) is returned to the Portal once the job has been completed. In this mode, a single WPS client can submit several concurrent job requests regardless the status of the previous jobs are. That is not the case in the synchronous mode.

The scheduler shown in the diagrams above dynamically allocate resources to the jobs. Depending on the processing centre this may be done in a slightly different way but following common principles:

- Concurrent processing is supported for a single request, limited by the available processing resources and within the limits configured for Urban TEP.
- Different requests of the same user or different users may be processed concurrently, splitting the resources dynamically.
- There is a scheduling strategy to ensure fairness. No request shall wait forever.
- For cloud bursting, on request more than the configured resources can be made available for Urban TEP jobs.
- Resources not used by Urban TEP at a certain time may be used by other non-Urban-TEP jobs.

.. _product_to_catalogue:

Product registration to catalogue
----------------------------------
   
.. uml::
  :caption: Product registration to catalogue
  :align: center
  
  participant "Staging"
  participant "Production Control" as PC
  participant "Catalogue"
  
  activate PC
  Staging -> PC : product URL(s) and metadata
  PC -> PC : collect and format metadata
  PC -> PC : instantiate catalogue client
  activate PC #Green
  PC -> Catalogue : send product metadata
  activate Catalogue
  Catalogue -> PC : metadata accepted
  deactivate Catalogue
  deactivate PC
  
The registration of product(s) to Catalogue occurs after a process has been successfully performed and its product(s) has been successfully staged. 
Production Control collects the metadata and formats it according to the Catalogue specification. A catalogue client is then instantiated to send the formatted metadata information to the Catalogue server. 
A confirmation that metadata information has been successfully accepted is returned to the Production Control.

Bulk processing and systematic processing
-----------------------------------------

Bulk processing is the processing of a larger set of input data with usually also a larger set of output data. This output data may serve as input to subsequent analysis, aggregation, or download. A user may pre-process a dataset at the platform and make it available as alternative to the publishing of the corresponding processing service to generate the same products on-demand. 

Asynchronous requests as described above can be used for bulk processing. The concept of a dataset collection (instead of single product granules) as input and the optional restriction in space and time period well-support bulk processing. In the processing centre bulk processing is handled by parallel processing on the cluster infrastructures. The result of bulk processing will be registered in the catalogue as a new dataset.

Systematic processing is the processing of any input product fulfilling a certain criterion, often in particular spatial coverage, and optionally including newly acquired products. This is relevant for datasets that are systematically ingested and where the processing result is of common interest to users. The decision on which products shall be generated systematically is a question of governance. 

The production and ingestion control component of some processing centre(s) contain functions to set up systematic processing. For systematically generated datasets no user request is required. The ingestion of new input data triggers the generation of new products for the target dataset. Users can access them as an existing dataset in the Urban TEP. Processing centres extend the collection automatically and update the catalogue. Subscription and NRT services on this basis - while out of scope for this phase of the Urban TEP - are in principle possible with this architecture. 

